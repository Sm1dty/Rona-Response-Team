{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texas State COVID Response Analysis\n",
    "This notebook contains the work to identify associations between the Texas state governmental response and the COVID-19 case count throughout the pandemic.\n",
    "\n",
    "\n",
    "## Data Cleanup\n",
    "As with most data mining projects, we will need to clean up the given data file in order to focus on the goal at hand. The \"all-states-history.csv\" file is a dataset of U.S. COVID-19 cases and deaths dating from the start of the pandemic to 11/29/20 and was sourced from [The Covid Tracking Project](https://covidtracking.com/data). \n",
    "\n",
    "In order to analyze with state policy actions, we will merge data from the [Oxford Covid-19 Government Response Tracker](https://github.com/OxCGRT/covid-policy-tracker) github dataset titled 'state-policies.csv'. Then we can encode  the data for market basket analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import squarify\n",
    "import seaborn as sns\n",
    "\n",
    "# for market basket analysis\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to a way to discretize and format data too be suitable for apriori analysis. The function below will discretize the necessary columns into bins and set the values to strings according to the bin they are placed in.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_data2(arr, k):\n",
    "    cols = arr.columns\n",
    "    for i in cols:\n",
    "        bin_range = pd.cut(arr[i],k)\n",
    "        col_copy = arr[i].astype('str')\n",
    "        for j in range(len(col_copy)):\n",
    "            col_copy[j] = i + \"\" +str(bin_range[j])\n",
    "            \n",
    "        arr[i] = col_copy   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3145: DtypeWarning: Columns (37) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# COVID tracking project data\n",
    "covid_data = pd.read_csv('all-states-history.csv')\n",
    "\n",
    "# state plicy data\n",
    "policy_data = pd.read_csv('state-policies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up Covid data to only include Minnesota instances and the appropriate attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>death</th>\n",
       "      <th>deathConfirmed</th>\n",
       "      <th>deathIncrease</th>\n",
       "      <th>hospitalized</th>\n",
       "      <th>hospitalizedIncrease</th>\n",
       "      <th>negative</th>\n",
       "      <th>negativeIncrease</th>\n",
       "      <th>positive</th>\n",
       "      <th>positiveIncrease</th>\n",
       "      <th>totalTestResults</th>\n",
       "      <th>totalTestResultsIncrease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-05</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-08</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>2020-11-25</td>\n",
       "      <td>TX</td>\n",
       "      <td>20950.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9188862.0</td>\n",
       "      <td>63830</td>\n",
       "      <td>1130980.0</td>\n",
       "      <td>15609</td>\n",
       "      <td>10319842.0</td>\n",
       "      <td>79439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>2020-11-26</td>\n",
       "      <td>TX</td>\n",
       "      <td>21156.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9242578.0</td>\n",
       "      <td>53716</td>\n",
       "      <td>1143616.0</td>\n",
       "      <td>12636</td>\n",
       "      <td>10386194.0</td>\n",
       "      <td>66352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>2020-11-27</td>\n",
       "      <td>TX</td>\n",
       "      <td>21207.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9351514.0</td>\n",
       "      <td>108936</td>\n",
       "      <td>1147045.0</td>\n",
       "      <td>3429</td>\n",
       "      <td>10498559.0</td>\n",
       "      <td>112365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>2020-11-28</td>\n",
       "      <td>TX</td>\n",
       "      <td>21309.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9485780.0</td>\n",
       "      <td>134266</td>\n",
       "      <td>1151069.0</td>\n",
       "      <td>4024</td>\n",
       "      <td>10636849.0</td>\n",
       "      <td>138290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2020-11-29</td>\n",
       "      <td>TX</td>\n",
       "      <td>21357.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9546404.0</td>\n",
       "      <td>60624</td>\n",
       "      <td>1157273.0</td>\n",
       "      <td>6204</td>\n",
       "      <td>10703677.0</td>\n",
       "      <td>66828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date state    death  deathConfirmed  deathIncrease  hospitalized  \\\n",
       "0   2020-03-04    TX      NaN             NaN              0           NaN   \n",
       "1   2020-03-05    TX      NaN             NaN              0           NaN   \n",
       "2   2020-03-06    TX      NaN             NaN              0           NaN   \n",
       "3   2020-03-07    TX      NaN             NaN              0           NaN   \n",
       "4   2020-03-08    TX      NaN             NaN              0           NaN   \n",
       "..         ...   ...      ...             ...            ...           ...   \n",
       "266 2020-11-25    TX  20950.0             NaN            200           NaN   \n",
       "267 2020-11-26    TX  21156.0             NaN            206           NaN   \n",
       "268 2020-11-27    TX  21207.0             NaN             51           NaN   \n",
       "269 2020-11-28    TX  21309.0             NaN            102           NaN   \n",
       "270 2020-11-29    TX  21357.0             NaN             48           NaN   \n",
       "\n",
       "     hospitalizedIncrease   negative  negativeIncrease   positive  \\\n",
       "0                       0        NaN                 0        1.0   \n",
       "1                       0        NaN                 0        1.0   \n",
       "2                       0        NaN                 0        5.0   \n",
       "3                       0        NaN                 0        8.0   \n",
       "4                       0        NaN                 0        8.0   \n",
       "..                    ...        ...               ...        ...   \n",
       "266                     0  9188862.0             63830  1130980.0   \n",
       "267                     0  9242578.0             53716  1143616.0   \n",
       "268                     0  9351514.0            108936  1147045.0   \n",
       "269                     0  9485780.0            134266  1151069.0   \n",
       "270                     0  9546404.0             60624  1157273.0   \n",
       "\n",
       "     positiveIncrease  totalTestResults  totalTestResultsIncrease  \n",
       "0                   0               1.0                         0  \n",
       "1                   0               1.0                         0  \n",
       "2                   4               5.0                         4  \n",
       "3                   3               8.0                         3  \n",
       "4                   0               8.0                         0  \n",
       "..                ...               ...                       ...  \n",
       "266             15609        10319842.0                     79439  \n",
       "267             12636        10386194.0                     66352  \n",
       "268              3429        10498559.0                    112365  \n",
       "269              4024        10636849.0                    138290  \n",
       "270              6204        10703677.0                     66828  \n",
       "\n",
       "[271 rows x 13 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#isolating the columns we need\n",
    "columns_to_show = ['date','state','death','deathConfirmed','deathIncrease','hospitalized','hospitalizedIncrease','negative'\n",
    "                   ,'negativeIncrease','positive','positiveIncrease','totalTestResults','totalTestResultsIncrease']\n",
    "\n",
    "#isolating only for MN data and putting in order March->November\n",
    "covid_clean_data = covid_data[covid_data['state'] == 'TX']\n",
    "covid_clean_data = covid_clean_data[columns_to_show]\n",
    "covid_clean_data = covid_clean_data.iloc[::-1]\n",
    "\n",
    "#reindexing for weekly processing \n",
    "covid_clean_data['date'] = covid_clean_data['date'].astype('datetime64[ns]')\n",
    "covid_clean_data = covid_clean_data.set_index('date')\n",
    "\n",
    "# isolating the columns that need to be summed when converting to weekly index\n",
    "increase_cols = covid_clean_data[['deathIncrease','hospitalizedIncrease','negativeIncrease','positiveIncrease','totalTestResultsIncrease']]\n",
    "#weekly_data = columns_to_sum.resample('W', label='right', closed='right').sum()\n",
    "increase_cols = increase_cols.reset_index(drop = True)\n",
    "\n",
    "# converting remaining non-sum columns to weekly index\n",
    "remaining_cols = covid_clean_data[['state','death','deathConfirmed','hospitalized', 'negative','positive','totalTestResults']]\n",
    "#remaining_cols = remaining_cols.resample('W').backfill().reset_index()\n",
    "#remaining_cols.head(39)\n",
    "\n",
    "#merging and resetting the datframe order to be more clear\n",
    "#covid_clean_data = pd.merge(remaining_cols, weekly_data, left_index = True, right_index = True).fillna(0)\n",
    "#covid_clean_data = covid_clean_data[['date','state','death','deathIncrease','deathConfirmed','hospitalized', 'hospitalizedIncrease','negative',\n",
    "                        #'negativeIncrease','positive', 'positiveIncrease','totalTestResults','totalTestResultsIncrease']]\n",
    "discretize_data2(increase_cols,4)\n",
    "covid_clean_data = covid_clean_data.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up state policy dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolating data only about the current state of interest, Minnesota\n",
    "policy_clean_data = policy_data[policy_data['RegionName'] == 'Texas']\n",
    "\n",
    "#declaring and extracting columns of interest from the original dataset\n",
    "columns_of_interest = ['Date', 'C1_School closing', 'C2_Workplace closing', \n",
    "                       'C3_Cancel public events', 'C4_Restrictions on gatherings', 'C6_Stay at home requirements', \n",
    "                       'C7_Restrictions on internal movement', 'C8_International travel controls', \n",
    "                       'H1_Public information campaigns', 'H2_Testing policy', 'H3_Contact tracing', \n",
    "                       'H4_Emergency investment in healthcare', \n",
    "                       'H6_Facial Coverings']\n",
    "policy_clean_data = policy_clean_data[columns_of_interest].fillna(0)\n",
    "\n",
    "# reformating date section and range of dates needed\n",
    "policy_clean_data = policy_clean_data.reset_index(drop = True)\n",
    "from datetime import datetime as dt\n",
    "\n",
    "date_col = policy_clean_data['Date'].astype('str')\n",
    "for i in range(policy_clean_data.shape[0]):\n",
    "    date_col[i] = dt.strptime(date_col[i], \"%Y%m%d\")\n",
    "    \n",
    "policy_clean_data['Date'] = date_col\n",
    "\n",
    "start_date = covid_clean_data['date'][0]\n",
    "end_date = dt.strptime('20201129', \"%Y%m%d\")\n",
    "\n",
    "policy_clean_data = policy_clean_data.rename(columns = {\"Date\": \"date\"})\n",
    "\n",
    "policy_clean_data = policy_clean_data.set_index('date')\n",
    "\n",
    "policy_clean_data = policy_clean_data[start_date:end_date].reset_index(drop=True)\n",
    "policy_clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing values of policy_clean_data to the type of policy each number corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C1_School closing\n",
    "policy_clean_data['C1_School closing'] = policy_clean_data['C1_School closing'].replace(0, 'no measures, school closing')\n",
    "policy_clean_data['C1_School closing'] = policy_clean_data['C1_School closing'].replace(1, 'recommend closing or all schools open with alterations resulting in significant differences compared to non-Covid-19 operations')\n",
    "policy_clean_data['C1_School closing'] = policy_clean_data['C1_School closing'].replace(2, 'require closing (only some levels or categories, eg just high school, or just public schools)')\n",
    "policy_clean_data['C1_School closing'] = policy_clean_data['C1_School closing'].replace(3, 'require closing all levels, school closing')\n",
    "#C2_Workplace closing\n",
    "policy_clean_data['C2_Workplace closing'] = policy_clean_data['C2_Workplace closing'].replace(0, 'no measures, workplace closing')\n",
    "policy_clean_data['C2_Workplace closing'] = policy_clean_data['C2_Workplace closing'].replace(1, 'recommend closing (or recommend work from home)')\n",
    "policy_clean_data['C2_Workplace closing'] = policy_clean_data['C2_Workplace closing'].replace(2, 'require closing (or work from home) for some sectors or categories of workers')\n",
    "policy_clean_data['C2_Workplace closing'] = policy_clean_data['C2_Workplace closing'].replace(3, 'require closing (or work from home) for all-but-essential workplaces (eg grocery stores, doctors)')\n",
    "#C3_Cancel public events\n",
    "policy_clean_data['C3_Cancel public events'] = policy_clean_data['C3_Cancel public events'].replace(0, 'no measures, cancel public events')\n",
    "policy_clean_data['C3_Cancel public events'] = policy_clean_data['C3_Cancel public events'].replace(1, 'recommend cancelling public events')\n",
    "policy_clean_data['C3_Cancel public events'] = policy_clean_data['C3_Cancel public events'].replace(2, 'require cancelling public events')\n",
    "#C4_Restrictions on gatherings                                                                                            \n",
    "policy_clean_data['C4_Restrictions on gatherings'] = policy_clean_data['C4_Restrictions on gatherings'].replace(0, 'no restrictions on gatherings')                                                                                              \n",
    "policy_clean_data['C4_Restrictions on gatherings'] = policy_clean_data['C4_Restrictions on gatherings'].replace(1, 'restrictions on very large gatherings (the limit is above 1000 people)')                                                                                              \n",
    "policy_clean_data['C4_Restrictions on gatherings'] = policy_clean_data['C4_Restrictions on gatherings'].replace(2, 'restrictions on gatherings between 101-1000 people')                                                                                              \n",
    "policy_clean_data['C4_Restrictions on gatherings'] = policy_clean_data['C4_Restrictions on gatherings'].replace(3, 'restrictions on gatherings between 11-100 people')                                                                                              \n",
    "policy_clean_data['C4_Restrictions on gatherings'] = policy_clean_data['C4_Restrictions on gatherings'].replace(4, 'restrictions on gatherings of 10 people or less')\n",
    "#C6_Stay at home requirements                                                                                            \n",
    "policy_clean_data['C6_Stay at home requirements'] = policy_clean_data['C6_Stay at home requirements'].replace(0, 'no measures for stay at home requirement')\n",
    "policy_clean_data['C6_Stay at home requirements'] = policy_clean_data['C6_Stay at home requirements'].replace(1, 'recommend not leaving house')\n",
    "policy_clean_data['C6_Stay at home requirements'] = policy_clean_data['C6_Stay at home requirements'].replace(2, 'require not leaving house with exceptions for daily exercise, grocery shopping, and \"essential\" trips')\n",
    "policy_clean_data['C6_Stay at home requirements'] = policy_clean_data['C6_Stay at home requirements'].replace(3, 'require not leaving house with minimal exceptions (eg allowed to leave once a week, or only one person can leave at a time, etc)')\n",
    "#C7_Restrictions on internal movement\n",
    "policy_clean_data['C7_Restrictions on internal movement'] = policy_clean_data['C7_Restrictions on internal movement'].replace(0, 'no measures on restrictions on internal movement')\n",
    "policy_clean_data['C7_Restrictions on internal movement'] = policy_clean_data['C7_Restrictions on internal movement'].replace(1, 'recommend not to travel between regions/cities')\n",
    "policy_clean_data['C7_Restrictions on internal movement'] = policy_clean_data['C7_Restrictions on internal movement'].replace(2, 'internal movement restrictions in place')\n",
    "#C8_International travel controls\n",
    "policy_clean_data['C8_International travel controls'] = policy_clean_data['C8_International travel controls'].replace(0, 'no restrictions on international travel')\n",
    "policy_clean_data['C8_International travel controls'] = policy_clean_data['C8_International travel controls'].replace(1, 'screening arrivals on international travel')\n",
    "policy_clean_data['C8_International travel controls'] = policy_clean_data['C8_International travel controls'].replace(2, 'quarantine arrivals from some or all regions for international travel')\n",
    "policy_clean_data['C8_International travel controls'] = policy_clean_data['C8_International travel controls'].replace(3, 'ban arrivals from some regions for international travel')\n",
    "policy_clean_data['C8_International travel controls'] = policy_clean_data['C8_International travel controls'].replace(4, 'ban on all regions or total border closure')\n",
    "#H1_Public information campaigns\n",
    "policy_clean_data['H1_Public information campaigns'] = policy_clean_data['H1_Public information campaigns'].replace(0, 'no Covid-19 public information campaign')\n",
    "policy_clean_data['H1_Public information campaigns'] = policy_clean_data['H1_Public information campaigns'].replace(1, 'public officials urging caution about Covid-19')\n",
    "policy_clean_data['H1_Public information campaigns'] = policy_clean_data['H1_Public information campaigns'].replace(2, 'coordinated public information campaign (eg across traditional and social media)')\n",
    "#H2_Testing policy\n",
    "policy_clean_data['H2_Testing policy'] = policy_clean_data['H2_Testing policy'].replace(0, 'no testing policy')\n",
    "policy_clean_data['H2_Testing policy'] = policy_clean_data['H2_Testing policy'].replace(1, 'only those who both (a) have symptoms AND (b) meet specific criteria (eg key workers, admitted to hospital, came into contact with a known case, returned from overseas)')\n",
    "policy_clean_data['H2_Testing policy'] = policy_clean_data['H2_Testing policy'].replace(2, 'testing of anyone showing Covid-19 symptoms')\n",
    "policy_clean_data['H2_Testing policy'] = policy_clean_data['H2_Testing policy'].replace(3, 'open public testing (eg \"drive through\" testing available to asymptomatic people)')\n",
    "#H3_Contact tracing\n",
    "policy_clean_data['H3_Contact tracing'] = policy_clean_data['H3_Contact tracing'].replace(0, 'no contact tracing')\n",
    "policy_clean_data['H3_Contact tracing'] = policy_clean_data['H3_Contact tracing'].replace(1, 'limited contact tracing; not done for all cases')\n",
    "policy_clean_data['H3_Contact tracing'] = policy_clean_data['H3_Contact tracing'].replace(2, 'comprehensive contact tracing; done for all identified cases')\n",
    "#H4_Emergency investment in healthcare\n",
    "### not a code, simply records the monetary value in USD\n",
    "policy_clean_data['H4_Emergency investment in healthcare'] = policy_clean_data['H4_Emergency investment in healthcare'].mask(policy_clean_data['H4_Emergency investment in healthcare'] > 0, 'investment')\n",
    "policy_clean_data['H4_Emergency investment in healthcare'] = policy_clean_data['H4_Emergency investment in healthcare'].replace(0, 'no new spending that day')\n",
    "\n",
    "#H6_Facial Coverings\n",
    "policy_clean_data['H6_Facial Coverings'] = policy_clean_data['H6_Facial Coverings'].replace(0, 'no policy on facial coverings')\n",
    "policy_clean_data['H6_Facial Coverings'] = policy_clean_data['H6_Facial Coverings'].replace(1, 'recommended facial coverings')\n",
    "policy_clean_data['H6_Facial Coverings'] = policy_clean_data['H6_Facial Coverings'].replace(2, 'facial coverings required in some specified shared/public spaces outside the home with other people present, or some situations when social distancing not possible')\n",
    "policy_clean_data['H6_Facial Coverings'] = policy_clean_data['H6_Facial Coverings'].replace(3, 'facial coverings required in all shared/public spaces outside the home with other people present or all situations when social distancing not possible')\n",
    "policy_clean_data['H6_Facial Coverings'] = policy_clean_data['H6_Facial Coverings'].replace(4, 'facial coverings required outside the home at all times regardless of location or presence of other people')\n",
    "                                                                                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging dataframes to create the basket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MN_basket = pd.merge(increase_cols, policy_clean_data, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Important MN Stats:\n",
    "\n",
    "- Population (mn.gov estimate): 5,680,337\n",
    "- Land Area (estimate): 79,610.08 sq. mi.\n",
    "- Population Density: 71.35 people/sq. mi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we jump into Apriori, it would be nice to get a vizual of the basket data, we will create a tree map similar to the one in Project 2's \"apriori analysis\" notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "all_elements = MN_basket.values.tolist()\n",
    "flat_list = [item for sublist in all_elements for item in sublist]\n",
    "temp_df = pd.DataFrame({'col':flat_list})\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (18, 7)\n",
    "color = plt.cm.copper(np.linspace(0, 1, 40))\n",
    "temp_df['col'].value_counts().head(40).plot.bar(color = color)\n",
    "plt.title('frequency of most popular items', fontsize = 20)\n",
    "plt.xticks(rotation = 90)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the most frequent items are:\n",
    "\n",
    "- limited contact tracing; not done for all cases\n",
    "- coordinated public information campaign (eg across traditional and social media)\n",
    "- negativeIncrease(-14092.25,39310.0]\n",
    "\n",
    "This doesn't provide too much information for how mandates relate to trends in cases, so lets do some association mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data set up as a series of 'transactions' with items (basket), we just need to encode the items and then use Apriori analysis and find association rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basket_array = MN_basket.values\n",
    "basket_cols = MN_basket.columns\n",
    "te = TransactionEncoder()\n",
    "te_dataset = te.fit(basket_array).transform(basket_array)\n",
    "\n",
    "MN_te_basket = pd.DataFrame(te_dataset, columns = te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MN_freq_itemsets = apriori(MN_te_basket, min_support=0.7, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MN_freq_itemsets['length'] = MN_freq_itemsets['itemsets'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(MN_freq_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "rules[\"antecedent_len\"] = rules[\"antecedents\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can mine for associations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len4 = rules[rules['antecedent_len'] >= 6]\n",
    "\n",
    "len4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_tracing = rules[(rules['antecedents'] == {'limited contact tracing; not done for all cases'})\n",
    "                    & (rules['antecedent support'] >= 0.9) \n",
    "                    & (rules['confidence'] >= 0.9)]\n",
    "len(contact_tracing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "school_restrictions = rules[(rules['antecedents'] == {'require closing all levels, school closing'})\n",
    "                    & (rules['antecedent support'] >= 0.9) \n",
    "                    & (rules['confidence'] >= 0.9)]\n",
    "\n",
    "school_restrictions.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_restrictions = rules[(rules['antecedents'] == {'recommend closing (or recommend work from home)'})\n",
    "                    & (rules['antecedent support'] >= 0.7) \n",
    "                    & (rules['confidence'] >= 0.7)]\n",
    "work_restrictions.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_events = rules[(rules['antecedents'] == {'require cancelling public events'})\n",
    "                    & (rules['antecedent support'] >= 0.7) \n",
    "                    & (rules['confidence'] >= 0.7)]\n",
    "public_events.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gathering_restrictions = rules[(rules['antecedents'] == {'restrictions on gatherings of 10 people or less'})\n",
    "                    & (rules['antecedent support'] >= 0.7) \n",
    "                    & (rules['confidence'] >= 0.7)]\n",
    "gathering_restrictions.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 38",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
